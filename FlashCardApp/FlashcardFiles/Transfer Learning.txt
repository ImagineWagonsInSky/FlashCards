{
  "What is transfer learning in NLP?": "Transfer learning involves leveraging knowledge from a pre-trained model on a large dataset to improve performance on a related task with limited data.",
  
  "What is pre-training in the context of NLP models?": "Pre-training refers to training a model on a large corpus to learn general language representations, often using self-supervised objectives like masked language modeling.",
  
  "What is fine-tuning in NLP?": "Fine-tuning involves adapting a pre-trained model to a specific downstream task by continuing training on task-specific data.",
  
  "How does BERT's architecture differ from GPT and T5?": "BERT uses an encoder-only architecture, GPT uses a decoder-only architecture, and T5 employs an encoder-decoder (sequence-to-sequence) architecture.",
  
  "What is BERT's training objective?": "BERT is trained using masked language modeling (MLM) and next sentence prediction (NSP) to capture bidirectional context.",
  
  "What is GPT's training objective?": "GPT is trained using causal (autoregressive) language modeling, predicting the next word in a sequence.",
  
  "What is T5's training objective?": "T5 is trained using a text-to-text approach, converting all NLP tasks into a unified text-to-text format.",
  
  "How does attention differ between BERT, GPT, and T5?": "BERT uses bidirectional self-attention, GPT uses unidirectional (causal) self-attention, and T5 uses both encoder self-attention and decoder cross-attention.",
  
  "What is masking in BERT?": "BERT uses random masking of input tokens during training to predict masked tokens, enabling bidirectional context learning.",
  
  "What is masking in GPT?": "GPT uses causal masking to prevent the model from attending to future tokens, ensuring unidirectional context.",
  
  "What is masking in T5?": "T5 uses span masking, where contiguous spans of text are masked and the model learns to predict the missing spans.",
  
  "What are the conceptual differences between pre-training and fine-tuning?": "Pre-training learns general language representations from large corpora, while fine-tuning adapts these representations to specific tasks using task-specific data.",
  
  "Why is transfer learning important in NLP?": "Transfer learning allows models to leverage knowledge from large datasets, improving performance on tasks with limited data and reducing training time.",
  
  "How does the encoder-decoder architecture benefit T5?": "The encoder-decoder architecture enables T5 to handle a wide range of tasks by encoding input sequences and generating output sequences in a unified framework.",
  
  "What are the advantages of using BERT for classification tasks?": "BERT's bidirectional context understanding makes it effective for tasks like sentiment analysis and question answering.",
  
  "What are the advantages of using GPT for text generation?": "GPT's autoregressive nature makes it well-suited for generating coherent and contextually relevant text.",
  
  "How does T5 unify different NLP tasks?": "T5 converts all tasks into a text-to-text format, allowing the same model architecture and training objective to be applied across diverse tasks.",
  
  "What is the role of self-supervision in pre-training?": "Self-supervision enables models to learn from unlabeled data by creating surrogate tasks, such as predicting masked tokens.",
  
  "How does fine-tuning affect a pre-trained model's performance?": "Fine-tuning adapts the model to specific tasks, improving performance by updating model parameters based on task-specific data.",
  
  "What are some challenges associated with fine-tuning?": "Challenges include overfitting to small datasets, catastrophic forgetting of pre-trained knowledge, and the need for careful hyperparameter tuning."
}
