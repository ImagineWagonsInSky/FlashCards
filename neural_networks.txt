{
    "What is a neural network?": "A network of small computing units that take input values, compute weighted sums, apply an activation function, and produce an output.",
    "Why are modern neural networks called \"deep learning\"?": "Because they have multiple hidden layers, allowing them to learn hierarchical representations of data.",
    "How do neural networks compare to logistic regression?": "Neural networks are more powerful classifiers, capable of learning any function given sufficient data and structure.",
    "What is the basic computation performed by a neural unit?": "A weighted sum of inputs plus a bias term, followed by a non-linear activation function.",
    "What is the formula for the weighted sum in vector notation?": "z = w · x + b",
    "What is the purpose of the activation function?": "To introduce non-linearity, enabling the network to learn complex patterns.",
    "What are three common activation functions in neural networks?": "Sigmoid, tanh, and ReLU (Rectified Linear Unit).",
    "What are the pros and cons of the sigmoid functioon?": "Pros: Squashes outputs to (0,1), useful for probabilities. Cons: Can suffer from the vanishing gradient problem.",
    "How does the tanh function compare to sigmoid?": "Similar shape but outputs range from (-1,1), which makes it centered around zero.",
    "Why is ReLU often preferred?": "It avoids the vanishing gradient problem by having a derivative of 1 for positive values.",
    "Why cant a single-layer perceptron solve the XOR problem?": "XOR is not linearly separable, and a single perceptron can only create linear decision boundaries.",
    "How can XOR be solved using neural networks?": "By adding a hidden layer, allowing the network to transform the input space into a linearly separable form.",
    "What is a feedforward neural network?": "A network where connections do not form cycles; outputs from one layer are passed only to the next layer.",
    "What is the general structure of a feedforward network?": "Input layer -> Hidden layer(s) -> Output layer",
    "What is a fully connected layer?": "A layer where each neuron is connected to every neuron in the previous and next layer.",
    "What is a multi-layer perceptron (MLP)?": "A feedforward neural network with at least one hidden layer (despite the name, it typically doesn't use perceptron activation functions).",
    "How is the output of a hidden layer computed?": "h = σ(Wx + b)",
    "What function is used for classification outputs?": "Softmax function: Converts a vector of real number into a probability distribution.",
    "What is the softmax formula?": "softmax(zi) = (exp(zi)) / (SUM(exp(zj)))",
    "Why must activation functions be non-linear?": "Without non-linearity, multiple layers collapse into an equivalent single-layer model.",
    "What is the goal of training a neural network?": "To find weights and biases that minimize the difference between predicted and actual outputs.",
    "What is the most common loss function for classification?": "Cross-entropy loss, which penalizes incorrect class probabilities.",
    "What is the formula for cross-entropy loss in a multi-class problem?": "L = -SUM_k(y_k log(\u0177k)), where yk is the true label (one-hot encoded) and \u0177k is the predicted probability.",
    "What algorithm is used to adjust weights?": "A Gradient descent, specifically a variant like stochastic gradient descent (SGD) or Adam.",
    "How are gradients computed in deep networks?": "Using backpropagation, which applies the chain rule to compute gradients layer by layer.",
    "What is the vanishing gradient problem?": "In deep networks with sigmoid or tanh activations, gradients shrink exponentially, making early layers hard to train.",
    "What is a dropout?": "A regularization technique where random units are ignored during training to prevent overfitting.",
    "How can neural networks be used for NLP classification?": "By using word embeddings as input features and training a feedforward classifier.",
    "What is a pretrained embedding?": "An embedding learned on large corpora (e.g., word2vec, GloVe) and used for downstream NLP tasks.",
    "What is mean pooling in NLP classification?": "Averaging the embeddings of all words in a text to form a single representation.",
    "What is a neural language model?": "A model that predicts the probability of the next word given previous words using a neural network.",
    "How does a feedforward neural LM differ from an n-gram model?": "Uses embeddings to capture word similarity and can handle longer contexts.",
    "How does a neural LM process input words?": "Converts them into one-hot vectors, retrieves embeddings, and processes them through hidden layers.",
    "What is self-training in neural LMs?": "The process where the model learns by predicting the next word in a large corpus without manual labels.",
    "What are the key benefits of neural language models?": "Better generalization to unseen text, ability to model long-range dependencies."
}